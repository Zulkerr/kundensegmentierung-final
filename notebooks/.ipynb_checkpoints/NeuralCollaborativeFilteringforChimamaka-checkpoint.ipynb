{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67a4d73d",
   "metadata": {},
   "source": [
    "# Neural Collaborative Filtering (NCF) - Vollständige Modulare Implementation\n",
    "\n",
    "Ein komplettes Neural Collaborative Filtering System für Empfehlungssysteme:\n",
    "- **User & Item Embeddings** für Matrix Factorization\n",
    "- **Generalized Matrix Factorization (GMF)** \n",
    "- **Multi-Layer Perceptron (MLP)** für nicht-lineare Interaktionen\n",
    "- **Neural Matrix Factorization (NeuMF)** - Kombination von GMF + MLP\n",
    "- Modular mit expliziten Layern und ausführlicher Dokumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52c298e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Imports / Bibliotheken importieren\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Tuple, List\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Device-Konfiguration / Configure computation device\n",
    "Device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {Device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63c0f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# USER EMBEDDING LAYER\n",
    "# Erstellt Embedding-Vektoren für User-IDs\n",
    "# ============================================================================\n",
    "\n",
    "class UserEmbeddingLayer(nn.Module):\n",
    "    # Initialisiert User-Embedding mit lernbaren Parametern\n",
    "    # Initialize user embedding with learnable parameters\n",
    "    def __init__(self, NumUsers: int, EmbeddingDim: int):\n",
    "        super(UserEmbeddingLayer, self).__init__()\n",
    "        \n",
    "        # Speichere Anzahl der User / Store number of users\n",
    "        self.NumUsers = NumUsers\n",
    "        \n",
    "        # Speichere Embedding-Dimension / Store embedding dimension\n",
    "        self.EmbeddingDim = EmbeddingDim\n",
    "        \n",
    "        # Erstelle User-Embedding-Layer / Create user embedding layer\n",
    "        self.UserEmbedding = nn.Embedding(\n",
    "            num_embeddings=NumUsers,      # Anzahl verschiedener User / Number of unique users\n",
    "            embedding_dim=EmbeddingDim    # Dimensionalität jedes Embeddings / Dimensionality of each embedding\n",
    "        )\n",
    "        \n",
    "        # Initialisiere Gewichte / Initialize weights\n",
    "        nn.init.normal_(self.UserEmbedding.weight, mean=0.0, std=0.01)\n",
    "    \n",
    "    # Forward-Pass: Hole User-Embeddings\n",
    "    # Forward pass: Retrieve user embeddings\n",
    "    def forward(self, UserIDs: torch.Tensor) -> torch.Tensor:\n",
    "        # Input: UserIDs mit Shape [BatchSize]\n",
    "        # Output: UserEmbeddings mit Shape [BatchSize, EmbeddingDim]\n",
    "        \n",
    "        # Führe Embedding-Lookup durch / Perform embedding lookup\n",
    "        UserEmbeddings = self.UserEmbedding(UserIDs)\n",
    "        \n",
    "        # Gebe User-Embeddings zurück / Return user embeddings\n",
    "        return UserEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af8f9d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ITEM EMBEDDING LAYER\n",
    "# Erstellt Embedding-Vektoren für Item-IDs\n",
    "# ============================================================================\n",
    "\n",
    "class ItemEmbeddingLayer(nn.Module):\n",
    "    # Initialisiert Item-Embedding mit lernbaren Parametern\n",
    "    # Initialize item embedding with learnable parameters\n",
    "    def __init__(self, NumItems: int, EmbeddingDim: int):\n",
    "        super(ItemEmbeddingLayer, self).__init__()\n",
    "        \n",
    "        # Speichere Anzahl der Items / Store number of items\n",
    "        self.NumItems = NumItems\n",
    "        \n",
    "        # Speichere Embedding-Dimension / Store embedding dimension\n",
    "        self.EmbeddingDim = EmbeddingDim\n",
    "        \n",
    "        # Erstelle Item-Embedding-Layer / Create item embedding layer\n",
    "        self.ItemEmbedding = nn.Embedding(\n",
    "            num_embeddings=NumItems,      # Anzahl verschiedener Items / Number of unique items\n",
    "            embedding_dim=EmbeddingDim    # Dimensionalität jedes Embeddings / Dimensionality of each embedding\n",
    "        )\n",
    "        \n",
    "        # Initialisiere Gewichte / Initialize weights\n",
    "        nn.init.normal_(self.ItemEmbedding.weight, mean=0.0, std=0.01)\n",
    "    \n",
    "    # Forward-Pass: Hole Item-Embeddings\n",
    "    # Forward pass: Retrieve item embeddings\n",
    "    def forward(self, ItemIDs: torch.Tensor) -> torch.Tensor:\n",
    "        # Input: ItemIDs mit Shape [BatchSize]\n",
    "        # Output: ItemEmbeddings mit Shape [BatchSize, EmbeddingDim]\n",
    "        \n",
    "        # Führe Embedding-Lookup durch / Perform embedding lookup\n",
    "        ItemEmbeddings = self.ItemEmbedding(ItemIDs)\n",
    "        \n",
    "        # Gebe Item-Embeddings zurück / Return item embeddings\n",
    "        return ItemEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "571dd8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GENERALIZED MATRIX FACTORIZATION (GMF)\n",
    "# Element-wise Produkt von User- und Item-Embeddings\n",
    "# ============================================================================\n",
    "\n",
    "class GeneralizedMatrixFactorization(nn.Module):\n",
    "    # Initialisiert GMF-Komponente\n",
    "    # Initialize GMF component\n",
    "    def __init__(self, NumUsers: int, NumItems: int, EmbeddingDim: int):\n",
    "        super(GeneralizedMatrixFactorization, self).__init__()\n",
    "        \n",
    "        # Speichere Konfiguration / Store configuration\n",
    "        self.NumUsers = NumUsers\n",
    "        self.NumItems = NumItems\n",
    "        self.EmbeddingDim = EmbeddingDim\n",
    "        \n",
    "        # Erstelle User-Embedding für GMF / Create user embedding for GMF\n",
    "        self.GMFUserEmbeddingLayer = UserEmbeddingLayer(\n",
    "            NumUsers=NumUsers,\n",
    "            EmbeddingDim=EmbeddingDim\n",
    "        )\n",
    "        \n",
    "        # Erstelle Item-Embedding für GMF / Create item embedding for GMF\n",
    "        self.GMFItemEmbeddingLayer = ItemEmbeddingLayer(\n",
    "            NumItems=NumItems,\n",
    "            EmbeddingDim=EmbeddingDim\n",
    "        )\n",
    "    \n",
    "    # Forward-Pass: Berechnet element-wise Produkt\n",
    "    # Forward pass: Compute element-wise product\n",
    "    def forward(self, UserIDs: torch.Tensor, ItemIDs: torch.Tensor) -> torch.Tensor:\n",
    "        # Input: UserIDs [BatchSize], ItemIDs [BatchSize]\n",
    "        # Output: GMF-Features [BatchSize, EmbeddingDim]\n",
    "        \n",
    "        # Hole User-Embeddings / Get user embeddings\n",
    "        UserEmbeddings = self.GMFUserEmbeddingLayer(UserIDs)\n",
    "        # Shape: [BatchSize, EmbeddingDim]\n",
    "        \n",
    "        # Hole Item-Embeddings / Get item embeddings\n",
    "        ItemEmbeddings = self.GMFItemEmbeddingLayer(ItemIDs)\n",
    "        # Shape: [BatchSize, EmbeddingDim]\n",
    "        \n",
    "        # Berechne element-wise Produkt (Hadamard-Produkt)\n",
    "        # Compute element-wise product (Hadamard product)\n",
    "        GMFOutput = UserEmbeddings * ItemEmbeddings\n",
    "        # Shape: [BatchSize, EmbeddingDim]\n",
    "        \n",
    "        # Gebe GMF-Output zurück / Return GMF output\n",
    "        return GMFOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfea3f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MULTI-LAYER PERCEPTRON (MLP) COMPONENT\n",
    "# Nicht-lineare Interaktionen zwischen User und Item\n",
    "# ============================================================================\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    # Initialisiert MLP-Komponente mit expliziten Layern\n",
    "    # Initialize MLP component with explicit layers\n",
    "    def __init__(self, NumUsers: int, NumItems: int, EmbeddingDim: int, \n",
    "                 MLPLayers: List[int], Dropout: float = 0.2):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        \n",
    "        # Speichere Konfiguration / Store configuration\n",
    "        self.NumUsers = NumUsers\n",
    "        self.NumItems = NumItems\n",
    "        self.EmbeddingDim = EmbeddingDim\n",
    "        self.MLPLayers = MLPLayers  # Liste von Hidden-Dimensionen / List of hidden dimensions\n",
    "        \n",
    "        # Erstelle User-Embedding für MLP / Create user embedding for MLP\n",
    "        self.MLPUserEmbeddingLayer = UserEmbeddingLayer(\n",
    "            NumUsers=NumUsers,\n",
    "            EmbeddingDim=EmbeddingDim\n",
    "        )\n",
    "        \n",
    "        # Erstelle Item-Embedding für MLP / Create item embedding for MLP\n",
    "        self.MLPItemEmbeddingLayer = ItemEmbeddingLayer(\n",
    "            NumItems=NumItems,\n",
    "            EmbeddingDim=EmbeddingDim\n",
    "        )\n",
    "        \n",
    "        # Berechne Input-Dimension für MLP (User + Item Embeddings konkateniert)\n",
    "        # Calculate input dimension for MLP (user + item embeddings concatenated)\n",
    "        InputDim = EmbeddingDim * 2\n",
    "        \n",
    "        # Erstelle explizite MLP-Layer / Create explicit MLP layers\n",
    "        # Layer 1: Input -> Erste Hidden-Schicht\n",
    "        self.MLPLinearLayer1 = nn.Linear(\n",
    "            in_features=InputDim,\n",
    "            out_features=MLPLayers[0],\n",
    "            bias=True\n",
    "        )\n",
    "        \n",
    "        # Layer 2: Erste Hidden -> Zweite Hidden\n",
    "        self.MLPLinearLayer2 = nn.Linear(\n",
    "            in_features=MLPLayers[0],\n",
    "            out_features=MLPLayers[1],\n",
    "            bias=True\n",
    "        )\n",
    "        \n",
    "        # Layer 3: Zweite Hidden -> Dritte Hidden\n",
    "        self.MLPLinearLayer3 = nn.Linear(\n",
    "            in_features=MLPLayers[1],\n",
    "            out_features=MLPLayers[2],\n",
    "            bias=True\n",
    "        )\n",
    "        \n",
    "        # Layer 4: Dritte Hidden -> Output\n",
    "        self.MLPLinearLayer4 = nn.Linear(\n",
    "            in_features=MLPLayers[2],\n",
    "            out_features=MLPLayers[3],\n",
    "            bias=True\n",
    "        )\n",
    "        \n",
    "        # Erstelle Dropout-Layer / Create dropout layers\n",
    "        self.DropoutLayer = nn.Dropout(p=Dropout)\n",
    "        \n",
    "        # Initialisiere Gewichte / Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    # Initialisiert MLP-Gewichte\n",
    "    # Initialize MLP weights\n",
    "    def _initialize_weights(self):\n",
    "        # Iteriere über alle Linear-Layer / Iterate over all linear layers\n",
    "        for Module in self.modules():\n",
    "            if isinstance(Module, nn.Linear):\n",
    "                # Xavier-Initialisierung / Xavier initialization\n",
    "                nn.init.xavier_uniform_(Module.weight)\n",
    "                if Module.bias is not None:\n",
    "                    nn.init.zeros_(Module.bias)\n",
    "    \n",
    "    # Forward-Pass: Berechnet MLP-Output\n",
    "    # Forward pass: Compute MLP output\n",
    "    def forward(self, UserIDs: torch.Tensor, ItemIDs: torch.Tensor) -> torch.Tensor:\n",
    "        # Input: UserIDs [BatchSize], ItemIDs [BatchSize]\n",
    "        # Output: MLP-Features [BatchSize, MLPLayers[-1]]\n",
    "        \n",
    "        # Hole User-Embeddings / Get user embeddings\n",
    "        UserEmbeddings = self.MLPUserEmbeddingLayer(UserIDs)\n",
    "        # Shape: [BatchSize, EmbeddingDim]\n",
    "        \n",
    "        # Hole Item-Embeddings / Get item embeddings\n",
    "        ItemEmbeddings = self.MLPItemEmbeddingLayer(ItemIDs)\n",
    "        # Shape: [BatchSize, EmbeddingDim]\n",
    "        \n",
    "        # Konkateniere User und Item Embeddings / Concatenate user and item embeddings\n",
    "        ConcatenatedEmbeddings = torch.cat([UserEmbeddings, ItemEmbeddings], dim=1)\n",
    "        # Shape: [BatchSize, EmbeddingDim * 2]\n",
    "        \n",
    "        # Layer 1 + ReLU + Dropout\n",
    "        Hidden = self.MLPLinearLayer1(ConcatenatedEmbeddings)\n",
    "        Hidden = F.relu(Hidden)\n",
    "        Hidden = self.DropoutLayer(Hidden)\n",
    "        \n",
    "        # Layer 2 + ReLU + Dropout\n",
    "        Hidden = self.MLPLinearLayer2(Hidden)\n",
    "        Hidden = F.relu(Hidden)\n",
    "        Hidden = self.DropoutLayer(Hidden)\n",
    "        \n",
    "        # Layer 3 + ReLU + Dropout\n",
    "        Hidden = self.MLPLinearLayer3(Hidden)\n",
    "        Hidden = F.relu(Hidden)\n",
    "        Hidden = self.DropoutLayer(Hidden)\n",
    "        \n",
    "        # Layer 4 (finale Schicht)\n",
    "        MLPOutput = self.MLPLinearLayer4(Hidden)\n",
    "        # Shape: [BatchSize, MLPLayers[-1]]\n",
    "        \n",
    "        # Gebe MLP-Output zurück / Return MLP output\n",
    "        return MLPOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0682c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NEURAL MATRIX FACTORIZATION (NeuMF)\n",
    "# Kombiniert GMF und MLP für leistungsstarke Empfehlungen\n",
    "# ============================================================================\n",
    "\n",
    "class NeuralMatrixFactorization(nn.Module):\n",
    "    # Initialisiert NeuMF-Modell\n",
    "    # Initialize NeuMF model\n",
    "    def __init__(self, NumUsers: int, NumItems: int, GMFEmbeddingDim: int = 64,\n",
    "                 MLPEmbeddingDim: int = 64, MLPLayers: List[int] = [128, 64, 32, 16],\n",
    "                 Dropout: float = 0.2, LearningRate: float = 0.001):\n",
    "        super(NeuralMatrixFactorization, self).__init__()\n",
    "        \n",
    "        # Speichere Konfiguration / Store configuration\n",
    "        self.NumUsers = NumUsers\n",
    "        self.NumItems = NumItems\n",
    "        self.GMFEmbeddingDim = GMFEmbeddingDim\n",
    "        self.MLPEmbeddingDim = MLPEmbeddingDim\n",
    "        self.LearningRate = LearningRate\n",
    "        \n",
    "        # Erstelle GMF-Komponente / Create GMF component\n",
    "        self.GMFComponent = GeneralizedMatrixFactorization(\n",
    "            NumUsers=NumUsers,\n",
    "            NumItems=NumItems,\n",
    "            EmbeddingDim=GMFEmbeddingDim\n",
    "        )\n",
    "        \n",
    "        # Erstelle MLP-Komponente / Create MLP component\n",
    "        self.MLPComponent = MultiLayerPerceptron(\n",
    "            NumUsers=NumUsers,\n",
    "            NumItems=NumItems,\n",
    "            EmbeddingDim=MLPEmbeddingDim,\n",
    "            MLPLayers=MLPLayers,\n",
    "            Dropout=Dropout\n",
    "        )\n",
    "        \n",
    "        # Berechne Dimension der konkatierten Features\n",
    "        # Calculate dimension of concatenated features\n",
    "        ConcatenatedDim = GMFEmbeddingDim + MLPLayers[-1]\n",
    "        \n",
    "        # Erstelle finale Vorhersage-Layer / Create final prediction layer\n",
    "        self.FinalPredictionLayer = nn.Linear(\n",
    "            in_features=ConcatenatedDim,\n",
    "            out_features=1,  # Binäre Vorhersage (Interaktion ja/nein) / Binary prediction\n",
    "            bias=True\n",
    "        )\n",
    "        \n",
    "        # Initialisiere finale Layer-Gewichte / Initialize final layer weights\n",
    "        nn.init.xavier_uniform_(self.FinalPredictionLayer.weight)\n",
    "        nn.init.zeros_(self.FinalPredictionLayer.bias)\n",
    "        \n",
    "        # Erstelle Optimizer / Create optimizer\n",
    "        self.Optimizer = torch.optim.Adam(\n",
    "            params=self.parameters(),\n",
    "            lr=LearningRate,\n",
    "            weight_decay=1e-5  # L2-Regularisierung / L2 regularization\n",
    "        )\n",
    "        \n",
    "        # Erstelle Loss-Funktion / Create loss function\n",
    "        self.LossFunction = nn.BCELoss(reduction='mean')\n",
    "    \n",
    "    # Forward-Pass: Berechnet Vorhersage-Score\n",
    "    # Forward pass: Compute prediction score\n",
    "    def forward(self, UserIDs: torch.Tensor, ItemIDs: torch.Tensor) -> torch.Tensor:\n",
    "        # Input: UserIDs [BatchSize], ItemIDs [BatchSize]\n",
    "        # Output: Predictions [BatchSize, 1]\n",
    "        \n",
    "        # Berechne GMF-Features / Compute GMF features\n",
    "        GMFFeatures = self.GMFComponent(UserIDs, ItemIDs)\n",
    "        # Shape: [BatchSize, GMFEmbeddingDim]\n",
    "        \n",
    "        # Berechne MLP-Features / Compute MLP features\n",
    "        MLPFeatures = self.MLPComponent(UserIDs, ItemIDs)\n",
    "        # Shape: [BatchSize, MLPLayers[-1]]\n",
    "        \n",
    "        # Konkateniere GMF und MLP Features / Concatenate GMF and MLP features\n",
    "        CombinedFeatures = torch.cat([GMFFeatures, MLPFeatures], dim=1)\n",
    "        # Shape: [BatchSize, GMFEmbeddingDim + MLPLayers[-1]]\n",
    "        \n",
    "        # Berechne finale Vorhersage / Compute final prediction\n",
    "        Logits = self.FinalPredictionLayer(CombinedFeatures)\n",
    "        # Shape: [BatchSize, 1]\n",
    "        \n",
    "        # Wende Sigmoid an für Wahrscheinlichkeit [0, 1]\n",
    "        # Apply sigmoid for probability [0, 1]\n",
    "        Predictions = torch.sigmoid(Logits)\n",
    "        \n",
    "        # Gebe Vorhersagen zurück / Return predictions\n",
    "        return Predictions\n",
    "    \n",
    "    # Führt einen einzelnen Trainings-Schritt durch\n",
    "    # Perform a single training step (Forward + Backward + Optimizer Step)\n",
    "    def train_step(self, \n",
    "                   UserIDs: torch.Tensor, \n",
    "                   ItemIDs: torch.Tensor, \n",
    "                   Labels: torch.Tensor,\n",
    "                   Device: torch.device) -> float:\n",
    "        \"\"\"\n",
    "        Führt einen einzelnen Trainings-Schritt durch\n",
    "        Perform a single training step\n",
    "        \n",
    "        Args:\n",
    "            UserIDs: Batch von User-IDs [BatchSize]\n",
    "            ItemIDs: Batch von Item-IDs [BatchSize]\n",
    "            Labels: Batch von Labels [BatchSize] (1 = Interaktion, 0 = keine)\n",
    "            Device: torch.device (cpu oder cuda)\n",
    "        \n",
    "        Returns:\n",
    "            Loss-Wert für diesen Batch / Loss value for this batch\n",
    "        \"\"\"\n",
    "        \n",
    "        # Setze Modell in Trainings-Modus / Set model to training mode\n",
    "        self.train()\n",
    "        \n",
    "        # Bewege Daten zu Device / Move data to device\n",
    "        UserIDs = UserIDs.to(Device)\n",
    "        ItemIDs = ItemIDs.to(Device)\n",
    "        Labels = Labels.to(Device).float().unsqueeze(1)  # Shape: [BatchSize, 1]\n",
    "        \n",
    "        # Setze Gradienten auf Null / Zero gradients\n",
    "        self.Optimizer.zero_grad()\n",
    "        \n",
    "        # Forward-Pass: Berechne Vorhersagen / Forward pass: compute predictions\n",
    "        Predictions = self.forward(UserIDs, ItemIDs)  # Shape: [BatchSize, 1]\n",
    "        \n",
    "        # Berechne Loss / Compute loss\n",
    "        Loss = self.LossFunction(Predictions, Labels)\n",
    "        \n",
    "        # Backward-Pass: Berechne Gradienten / Backward pass: compute gradients\n",
    "        Loss.backward()\n",
    "        \n",
    "        # Optimizer-Schritt: Update Gewichte / Optimizer step: update weights\n",
    "        self.Optimizer.step()\n",
    "        \n",
    "        # Gebe Loss-Wert zurück / Return loss value\n",
    "        return Loss.item()\n",
    "    \n",
    "    # Trainiert das Modell über mehrere Epochen\n",
    "    # Train the model over multiple epochs\n",
    "    def train_model(self,\n",
    "                    TrainDataLoader,\n",
    "                    NumEpochs: int,\n",
    "                    Device: torch.device,\n",
    "                    ValidationDataLoader = None,\n",
    "                    PrintEverySteps: int = 100) -> Tuple[List[float], List[float]]:\n",
    "        \"\"\"\n",
    "        Trainiert das NCF-Modell über mehrere Epochen\n",
    "        Train the NCF model over multiple epochs\n",
    "        \n",
    "        Args:\n",
    "            TrainDataLoader: DataLoader mit Trainings-Daten / Training data loader\n",
    "            NumEpochs: Anzahl der Epochen / Number of epochs\n",
    "            Device: torch.device (cpu oder cuda)\n",
    "            ValidationDataLoader: Optional DataLoader für Validierung / Optional validation data loader\n",
    "            PrintEverySteps: Drucke alle N Batches / Print every N batches\n",
    "        \n",
    "        Returns:\n",
    "            (TrainingLossHistory, ValidationLossHistory) - Listen mit Loss-Werten pro Epoche\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # Initialisiere Listen für Verlauf / Initialize history lists\n",
    "        TrainingLossHistory = []\n",
    "        ValidationLossHistory = []\n",
    "        ValidationAccuracyHistory = []\n",
    "        \n",
    "        # Iteriere über alle Epochen / Iterate over all epochs\n",
    "        for Epoch in range(NumEpochs):\n",
    "            # Initialisiere Epoch-Loss / Initialize epoch loss\n",
    "            EpochLoss = 0.0\n",
    "            NumBatches = 0\n",
    "            \n",
    "            # Iteriere über alle Batches / Iterate over all batches\n",
    "            for BatchIdx, (UserIDs, ItemIDs, Labels) in enumerate(TrainDataLoader):\n",
    "                # Führe Trainings-Schritt durch / Perform training step\n",
    "                BatchLoss = self.train_step(\n",
    "                    UserIDs=UserIDs,\n",
    "                    ItemIDs=ItemIDs,\n",
    "                    Labels=Labels,\n",
    "                    Device=Device\n",
    "                )\n",
    "                \n",
    "                # Akkumuliere Loss / Accumulate loss\n",
    "                EpochLoss += BatchLoss\n",
    "                NumBatches += 1\n",
    "                \n",
    "                # Drucke Statistiken / Print statistics\n",
    "                if (BatchIdx + 1) % PrintEverySteps == 0:\n",
    "                    AvgLoss = EpochLoss / NumBatches\n",
    "                    print(f\"Epoch [{Epoch+1}/{NumEpochs}] | \"\n",
    "                          f\"Batch [{BatchIdx+1}/{len(TrainDataLoader)}] | \"\n",
    "                          f\"Loss: {BatchLoss:.4f} | \"\n",
    "                          f\"Avg Loss: {AvgLoss:.4f}\")\n",
    "            \n",
    "            # Berechne durchschnittlichen Epoch-Loss / Calculate average epoch loss\n",
    "            AvgEpochLoss = EpochLoss / NumBatches\n",
    "            TrainingLossHistory.append(AvgEpochLoss)\n",
    "            \n",
    "            # Validierung falls DataLoader gegeben / Validation if dataloader provided\n",
    "            if ValidationDataLoader is not None:\n",
    "                # Setze Modell in Evaluations-Modus / Set model to evaluation mode\n",
    "                self.eval()\n",
    "                \n",
    "                # Initialisiere Validierungs-Metriken / Initialize validation metrics\n",
    "                ValLoss = 0.0\n",
    "                ValBatches = 0\n",
    "                CorrectPredictions = 0\n",
    "                TotalSamples = 0\n",
    "                \n",
    "                # Deaktiviere Gradienten / Disable gradients\n",
    "                with torch.no_grad():\n",
    "                    for UserIDs, ItemIDs, Labels in ValidationDataLoader:\n",
    "                        # Bewege zu Device / Move to device\n",
    "                        UserIDs = UserIDs.to(Device)\n",
    "                        ItemIDs = ItemIDs.to(Device)\n",
    "                        Labels = Labels.to(Device).float().unsqueeze(1)\n",
    "                        \n",
    "                        # Forward-Pass / Forward pass\n",
    "                        Predictions = self.forward(UserIDs, ItemIDs)\n",
    "                        \n",
    "                        # Berechne Loss / Compute loss\n",
    "                        Loss = self.LossFunction(Predictions, Labels)\n",
    "                        ValLoss += Loss.item()\n",
    "                        \n",
    "                        # Berechne Accuracy / Compute accuracy\n",
    "                        BinaryPredictions = (Predictions > 0.5).float()\n",
    "                        CorrectPredictions += (BinaryPredictions == Labels).sum().item()\n",
    "                        TotalSamples += Labels.size(0)\n",
    "                        \n",
    "                        ValBatches += 1\n",
    "                \n",
    "                # Berechne durchschnittliche Metriken / Calculate average metrics\n",
    "                AvgValLoss = ValLoss / ValBatches\n",
    "                ValAccuracy = CorrectPredictions / TotalSamples\n",
    "                \n",
    "                ValidationLossHistory.append(AvgValLoss)\n",
    "                ValidationAccuracyHistory.append(ValAccuracy)\n",
    "                \n",
    "                print(f\"\\nEpoch [{Epoch+1}/{NumEpochs}] Summary:\")\n",
    "                print(f\"  Training Loss:     {AvgEpochLoss:.4f}\")\n",
    "                print(f\"  Validation Loss:   {AvgValLoss:.4f}\")\n",
    "                print(f\"  Validation Acc:    {ValAccuracy:.4f}\")\n",
    "            else:\n",
    "                print(f\"\\nEpoch [{Epoch+1}/{NumEpochs}] Summary:\")\n",
    "                print(f\"  Training Loss: {AvgEpochLoss:.4f}\")\n",
    "            \n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "        \n",
    "        # Gebe Historie zurück / Return history\n",
    "        return TrainingLossHistory, ValidationLossHistory\n",
    "    \n",
    "    # Generiert Top-K Empfehlungen für einen Benutzer\n",
    "    # Generate Top-K recommendations for a user\n",
    "    def recommend(self,\n",
    "                  UserID: int,\n",
    "                  NumItems: int,\n",
    "                  TopK: int,\n",
    "                  Device: torch.device) -> List[Tuple[int, float]]:\n",
    "        \"\"\"\n",
    "        Generiert Top-K Empfehlungen für einen Benutzer\n",
    "        Generate top-K recommendations for a user\n",
    "        \n",
    "        Args:\n",
    "            UserID: ID des Benutzers / User ID\n",
    "            NumItems: Anzahl aller Items / Total number of items\n",
    "            TopK: Anzahl der Top-Empfehlungen / Number of top recommendations\n",
    "            Device: torch.device (cpu oder cuda)\n",
    "        \n",
    "        Returns:\n",
    "            Liste von (ItemID, Score) Tupeln / List of (ItemID, Score) tuples\n",
    "        \"\"\"\n",
    "        \n",
    "        # Setze Modell in Evaluations-Modus / Set model to evaluation mode\n",
    "        self.eval()\n",
    "        \n",
    "        # Erstelle Tensoren für alle Items / Create tensors for all items\n",
    "        AllItemIDs = list(range(NumItems))\n",
    "        UserIDs = torch.tensor([UserID] * NumItems, dtype=torch.long)\n",
    "        ItemIDs = torch.tensor(AllItemIDs, dtype=torch.long)\n",
    "        \n",
    "        # Bewege zu Device / Move to device\n",
    "        UserIDs = UserIDs.to(Device)\n",
    "        ItemIDs = ItemIDs.to(Device)\n",
    "        \n",
    "        # Deaktiviere Gradienten / Disable gradients\n",
    "        with torch.no_grad():\n",
    "            # Berechne Vorhersagen für alle Items / Compute predictions for all items\n",
    "            Predictions = self.forward(UserIDs, ItemIDs)\n",
    "            Predictions = Predictions.squeeze().cpu().numpy()\n",
    "        \n",
    "        # Erstelle Liste von (ItemID, Score) / Create list of (ItemID, Score)\n",
    "        ItemScores = list(zip(AllItemIDs, Predictions))\n",
    "        \n",
    "        # Sortiere nach Score absteigend / Sort by score descending\n",
    "        ItemScores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Gebe Top-K Empfehlungen zurück / Return top-K recommendations\n",
    "        return ItemScores[:TopK]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7391374",
   "metadata": {},
   "source": [
    "## Anwendungsbeispiel: Fake Empfehlungssystem\n",
    "\n",
    "Fake daten erstellen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3aebec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Users:        1,000\n",
      "Anzahl Items:        500\n",
      "Anzahl Interaktionen: 9,910\n",
      "Positive Samples:    2,710\n",
      "Negative Samples:    7,200\n",
      "Positive Rate:       27.35%\n",
      "\n",
      "Erste 10 Zeilen:\n",
      "   UserID  ItemID  Label\n",
      "0     102     441      0\n",
      "1     435     278      0\n",
      "2     860     250      0\n",
      "3     270     309      0\n",
      "4     106     207      1\n",
      "5      71     320      0\n",
      "6     700     139      0\n",
      "7      20     279      0\n",
      "8     614      33      0\n",
      "9     121     308      0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SCHRITT 1: SYNTHETISCHE DATEN GENERIEREN\n",
    "# Erstellt einen fake Film-Bewertungs-Datensatz\n",
    "# ============================================================================\n",
    "\n",
    "# Erstelle synthetische Daten / Create synthetic data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Konfiguration / Configuration\n",
    "NumUsers = 1000           # Anzahl der User / Number of users\n",
    "NumItems = 500            # Anzahl der Items (Filme) / Number of items (movies)\n",
    "NumInteractions = 10000   # Anzahl der Interaktionen / Number of interactions\n",
    "\n",
    "# Generiere zufällige User-Item-Interaktionen / Generate random user-item interactions\n",
    "UserIDs = np.random.randint(0, NumUsers, size=NumInteractions)\n",
    "ItemIDs = np.random.randint(0, NumItems, size=NumInteractions)\n",
    "\n",
    "# Generiere Labels (1 = positive Interaktion, 0 = negative)\n",
    "# Generate labels (1 = positive interaction, 0 = negative)\n",
    "# Simuliere dass beliebte Items höhere Wahrscheinlichkeit haben\n",
    "# Simulate that popular items have higher probability\n",
    "ItemPopularity = np.random.beta(2, 5, size=NumItems)  # Schiefe Verteilung / Skewed distribution\n",
    "Labels = []\n",
    "\n",
    "for UserID, ItemID in zip(UserIDs, ItemIDs):\n",
    "    # Positive Interaktion mit Wahrscheinlichkeit basierend auf Item-Popularität\n",
    "    # Positive interaction with probability based on item popularity\n",
    "    Probability = ItemPopularity[ItemID]\n",
    "    Label = 1 if np.random.random() < Probability else 0\n",
    "    Labels.append(Label)\n",
    "\n",
    "Labels = np.array(Labels)\n",
    "\n",
    "# Erstelle DataFrame / Create dataframe\n",
    "Data = pd.DataFrame({\n",
    "    'UserID': UserIDs,\n",
    "    'ItemID': ItemIDs,\n",
    "    'Label': Labels\n",
    "})\n",
    "\n",
    "# Entferne Duplikate / Remove duplicates\n",
    "Data = Data.drop_duplicates(subset=['UserID', 'ItemID'])\n",
    "\n",
    "\n",
    "print(f\"Anzahl Users:        {NumUsers:,}\")\n",
    "print(f\"Anzahl Items:        {NumItems:,}\")\n",
    "print(f\"Anzahl Interaktionen: {len(Data):,}\")\n",
    "print(f\"Positive Samples:    {(Data['Label'] == 1).sum():,}\")\n",
    "print(f\"Negative Samples:    {(Data['Label'] == 0).sum():,}\")\n",
    "print(f\"Positive Rate:       {(Data['Label'] == 1).mean():.2%}\")\n",
    "print(\"\\nErste 10 Zeilen:\")\n",
    "print(Data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86498be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training-Samples: 7,928\n",
      "Test-Samples:     1,982\n",
      "\n",
      " DataLoader erstellt\n",
      "  Batch-Größe:        256\n",
      "  Training-Batches:   31\n",
      "  Test-Batches:       8\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SCHRITT 2: DATEN VORBEREITEN UND AUFTEILEN\n",
    "# Train-Test-Split und DataLoader erstellen\n",
    "# ============================================================================\n",
    "\n",
    "# Teile Daten in Training und Test / Split data into training and test\n",
    "TrainData, TestData = train_test_split(\n",
    "    Data, \n",
    "    test_size=0.2,      # 20% für Test / 20% for test\n",
    "    random_state=42,\n",
    "    stratify=Data['Label']  # Stratifiziere nach Labels / Stratify by labels\n",
    ")\n",
    "\n",
    "print(f\"Training-Samples: {len(TrainData):,}\")\n",
    "print(f\"Test-Samples:     {len(TestData):,}\")\n",
    "\n",
    "# Erstelle PyTorch-Dataset / Create PyTorch dataset\n",
    "class NCFDataset(torch.utils.data.Dataset):\n",
    "    # Initialisiert Dataset\n",
    "    # Initialize dataset\n",
    "    def __init__(self, DataFrame):\n",
    "        # Speichere User-IDs / Store user IDs\n",
    "        self.UserIDs = torch.tensor(DataFrame['UserID'].values, dtype=torch.long)\n",
    "        \n",
    "        # Speichere Item-IDs / Store item IDs\n",
    "        self.ItemIDs = torch.tensor(DataFrame['ItemID'].values, dtype=torch.long)\n",
    "        \n",
    "        # Speichere Labels / Store labels\n",
    "        self.Labels = torch.tensor(DataFrame['Label'].values, dtype=torch.long)\n",
    "    \n",
    "    # Gibt Länge des Datasets zurück\n",
    "    # Return length of dataset\n",
    "    def __len__(self):\n",
    "        return len(self.UserIDs)\n",
    "    \n",
    "    # Gibt einzelnes Sample zurück\n",
    "    # Return single sample\n",
    "    def __getitem__(self, Index):\n",
    "        return self.UserIDs[Index], self.ItemIDs[Index], self.Labels[Index]\n",
    "\n",
    "# Erstelle Datasets / Create datasets\n",
    "TrainDataset = NCFDataset(TrainData)\n",
    "TestDataset = NCFDataset(TestData)\n",
    "\n",
    "# Erstelle DataLoader / Create dataloaders\n",
    "BatchSize = 256\n",
    "\n",
    "TrainDataLoader = torch.utils.data.DataLoader(\n",
    "    TrainDataset,\n",
    "    batch_size=BatchSize,\n",
    "    shuffle=True,        # Mische Trainings-Daten / Shuffle training data\n",
    "    num_workers=0        # Anzahl Worker-Prozesse / Number of worker processes\n",
    ")\n",
    "\n",
    "TestDataLoader = torch.utils.data.DataLoader(\n",
    "    TestDataset,\n",
    "    batch_size=BatchSize,\n",
    "    shuffle=False,       # Mische Test-Daten nicht / Don't shuffle test data\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"\\n DataLoader erstellt\")\n",
    "print(f\"  Batch-Größe:        {BatchSize}\")\n",
    "print(f\"  Training-Batches:   {len(TrainDataLoader)}\")\n",
    "print(f\"  Test-Batches:       {len(TestDataLoader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b817cf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SCHRITT 3: MODELL INITIALISIEREN\n",
    "# Erstellt das NeuMF-Modell mit Hyperparametern\n",
    "# ============================================================================\n",
    "\n",
    "# Definiere Hyperparameter / Define hyperparameters\n",
    "GMFEmbeddingDim = 64        # Embedding-Dimension für GMF / Embedding dimension for GMF\n",
    "MLPEmbeddingDim = 64        # Embedding-Dimension für MLP / Embedding dimension for MLP\n",
    "MLPLayers = [128, 64, 32, 16]  # MLP-Layer-Größen / MLP layer sizes\n",
    "Dropout = 0.2               # Dropout-Rate / Dropout rate\n",
    "LearningRate = 0.001        # Lernrate / Learning rate\n",
    "\n",
    "# Erstelle NeuMF-Modell / Create NeuMF model\n",
    "NCFModel = NeuralMatrixFactorization(\n",
    "    NumUsers=NumUsers,\n",
    "    NumItems=NumItems,\n",
    "    GMFEmbeddingDim=GMFEmbeddingDim,\n",
    "    MLPEmbeddingDim=MLPEmbeddingDim,\n",
    "    MLPLayers=MLPLayers,\n",
    "    Dropout=Dropout,\n",
    "    LearningRate=LearningRate  # Optimizer und Loss werden automatisch erstellt\n",
    ")\n",
    "\n",
    "# Zähle Modell-Parameter / Count model parameters\n",
    "TotalParams = sum(p.numel() for p in NCFModel.parameters())\n",
    "TrainableParams = sum(p.numel() for p in NCFModel.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38b8a51",
   "metadata": {},
   "source": [
    "Man brauch das meist nie aber kann ja nicht schaden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0ddb7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEURAL COLLABORATIVE FILTERING MODEL INITIALISIERT\n",
      "Number of Users:          1,000\n",
      "Number of Items:          500\n",
      "GMF Embedding Dimension:  64\n",
      "MLP Embedding Dimension:  64\n",
      "MLP Layers:               [128, 64, 32, 16]\n",
      "Dropout Rate:             0.2\n",
      "Learning Rate:            0.001\n",
      "\n",
      "Total Parameters:         219,457\n",
      "Trainable Parameters:     219,457\n",
      "\n",
      " MODELL-KOMPONENTEN:\n",
      "  ├─ GMF Component:\n",
      "  │   ├─ GMFUserEmbeddingLayer:  64,000 Parameter\n",
      "  │   └─ GMFItemEmbeddingLayer:  32,000 Parameter\n",
      "  │\n",
      "  ├─ MLP Component:\n",
      "  │   ├─ MLPUserEmbeddingLayer:  64,000 Parameter\n",
      "  │   ├─ MLPItemEmbeddingLayer:  32,000 Parameter\n",
      "  │   ├─ MLPLinearLayer1:        16,512 Parameter\n",
      "  │   ├─ MLPLinearLayer2:        8,256 Parameter\n",
      "  │   ├─ MLPLinearLayer3:        2,080 Parameter\n",
      "  │   └─ MLPLinearLayer4:        528 Parameter\n",
      "  │\n",
      "  └─ FinalPredictionLayer:       81 Parameter\n"
     ]
    }
   ],
   "source": [
    "print(\"NEURAL COLLABORATIVE FILTERING MODEL INITIALISIERT\")\n",
    "print(f\"Number of Users:          {NumUsers:,}\")\n",
    "print(f\"Number of Items:          {NumItems:,}\")\n",
    "print(f\"GMF Embedding Dimension:  {GMFEmbeddingDim}\")\n",
    "print(f\"MLP Embedding Dimension:  {MLPEmbeddingDim}\")\n",
    "print(f\"MLP Layers:               {MLPLayers}\")\n",
    "print(f\"Dropout Rate:             {Dropout}\")\n",
    "print(f\"Learning Rate:            {LearningRate}\")\n",
    "print(f\"\\nTotal Parameters:         {TotalParams:,}\")\n",
    "print(f\"Trainable Parameters:     {TrainableParams:,}\")\n",
    "\n",
    "# Zeige Modell-Architektur / Show model architecture\n",
    "print(\"\\n MODELL-KOMPONENTEN:\")\n",
    "print(f\"  ├─ GMF Component:\")\n",
    "print(f\"  │   ├─ GMFUserEmbeddingLayer:  {sum(p.numel() for p in NCFModel.GMFComponent.GMFUserEmbeddingLayer.parameters()):,} Parameter\")\n",
    "print(f\"  │   └─ GMFItemEmbeddingLayer:  {sum(p.numel() for p in NCFModel.GMFComponent.GMFItemEmbeddingLayer.parameters()):,} Parameter\")\n",
    "print(f\"  │\")\n",
    "print(f\"  ├─ MLP Component:\")\n",
    "print(f\"  │   ├─ MLPUserEmbeddingLayer:  {sum(p.numel() for p in NCFModel.MLPComponent.MLPUserEmbeddingLayer.parameters()):,} Parameter\")\n",
    "print(f\"  │   ├─ MLPItemEmbeddingLayer:  {sum(p.numel() for p in NCFModel.MLPComponent.MLPItemEmbeddingLayer.parameters()):,} Parameter\")\n",
    "print(f\"  │   ├─ MLPLinearLayer1:        {sum(p.numel() for p in NCFModel.MLPComponent.MLPLinearLayer1.parameters()):,} Parameter\")\n",
    "print(f\"  │   ├─ MLPLinearLayer2:        {sum(p.numel() for p in NCFModel.MLPComponent.MLPLinearLayer2.parameters()):,} Parameter\")\n",
    "print(f\"  │   ├─ MLPLinearLayer3:        {sum(p.numel() for p in NCFModel.MLPComponent.MLPLinearLayer3.parameters()):,} Parameter\")\n",
    "print(f\"  │   └─ MLPLinearLayer4:        {sum(p.numel() for p in NCFModel.MLPComponent.MLPLinearLayer4.parameters()):,} Parameter\")\n",
    "print(f\"  │\")\n",
    "print(f\"  └─ FinalPredictionLayer:       {sum(p.numel() for p in NCFModel.FinalPredictionLayer.parameters()):,} Parameter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b9e7d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] | Batch [10/31] | Loss: 0.6762 | Avg Loss: 0.6863\n",
      "Epoch [1/5] | Batch [20/31] | Loss: 0.6585 | Avg Loss: 0.6749\n",
      "Epoch [1/5] | Batch [30/31] | Loss: 0.5914 | Avg Loss: 0.6578\n",
      "\n",
      "Epoch [1/5] Summary:\n",
      "  Training Loss:     0.6555\n",
      "  Validation Loss:   0.5807\n",
      "  Validation Acc:    0.7265\n",
      "Epoch [2/5] | Batch [10/31] | Loss: 0.5613 | Avg Loss: 0.5613\n",
      "Epoch [2/5] | Batch [20/31] | Loss: 0.5585 | Avg Loss: 0.5481\n",
      "Epoch [2/5] | Batch [30/31] | Loss: 0.5162 | Avg Loss: 0.5451\n",
      "\n",
      "Epoch [2/5] Summary:\n",
      "  Training Loss:     0.5465\n",
      "  Validation Loss:   0.5517\n",
      "  Validation Acc:    0.7265\n",
      "Epoch [3/5] | Batch [10/31] | Loss: 0.4936 | Avg Loss: 0.4952\n",
      "Epoch [3/5] | Batch [20/31] | Loss: 0.4906 | Avg Loss: 0.4871\n",
      "Epoch [3/5] | Batch [30/31] | Loss: 0.4418 | Avg Loss: 0.4920\n",
      "\n",
      "Epoch [3/5] Summary:\n",
      "  Training Loss:     0.4911\n",
      "  Validation Loss:   0.5858\n",
      "  Validation Acc:    0.7265\n",
      "Epoch [2/5] | Batch [10/31] | Loss: 0.5613 | Avg Loss: 0.5613\n",
      "Epoch [2/5] | Batch [20/31] | Loss: 0.5585 | Avg Loss: 0.5481\n",
      "Epoch [2/5] | Batch [30/31] | Loss: 0.5162 | Avg Loss: 0.5451\n",
      "\n",
      "Epoch [2/5] Summary:\n",
      "  Training Loss:     0.5465\n",
      "  Validation Loss:   0.5517\n",
      "  Validation Acc:    0.7265\n",
      "Epoch [3/5] | Batch [10/31] | Loss: 0.4936 | Avg Loss: 0.4952\n",
      "Epoch [3/5] | Batch [20/31] | Loss: 0.4906 | Avg Loss: 0.4871\n",
      "Epoch [3/5] | Batch [30/31] | Loss: 0.4418 | Avg Loss: 0.4920\n",
      "\n",
      "Epoch [3/5] Summary:\n",
      "  Training Loss:     0.4911\n",
      "  Validation Loss:   0.5858\n",
      "  Validation Acc:    0.7265\n",
      "Epoch [4/5] | Batch [10/31] | Loss: 0.4512 | Avg Loss: 0.4490\n",
      "Epoch [4/5] | Batch [20/31] | Loss: 0.5326 | Avg Loss: 0.4480\n",
      "Epoch [4/5] | Batch [30/31] | Loss: 0.4601 | Avg Loss: 0.4623\n",
      "\n",
      "Epoch [4/5] Summary:\n",
      "  Training Loss:     0.4628\n",
      "  Validation Loss:   0.6043\n",
      "  Validation Acc:    0.6771\n",
      "Epoch [5/5] | Batch [10/31] | Loss: 0.4259 | Avg Loss: 0.4358\n",
      "Epoch [5/5] | Batch [20/31] | Loss: 0.4574 | Avg Loss: 0.4415\n",
      "Epoch [5/5] | Batch [30/31] | Loss: 0.4917 | Avg Loss: 0.4446\n",
      "Epoch [4/5] | Batch [10/31] | Loss: 0.4512 | Avg Loss: 0.4490\n",
      "Epoch [4/5] | Batch [20/31] | Loss: 0.5326 | Avg Loss: 0.4480\n",
      "Epoch [4/5] | Batch [30/31] | Loss: 0.4601 | Avg Loss: 0.4623\n",
      "\n",
      "Epoch [4/5] Summary:\n",
      "  Training Loss:     0.4628\n",
      "  Validation Loss:   0.6043\n",
      "  Validation Acc:    0.6771\n",
      "Epoch [5/5] | Batch [10/31] | Loss: 0.4259 | Avg Loss: 0.4358\n",
      "Epoch [5/5] | Batch [20/31] | Loss: 0.4574 | Avg Loss: 0.4415\n",
      "Epoch [5/5] | Batch [30/31] | Loss: 0.4917 | Avg Loss: 0.4446\n",
      "\n",
      "Epoch [5/5] Summary:\n",
      "  Training Loss:     0.4445\n",
      "  Validation Loss:   0.6453\n",
      "  Validation Acc:    0.6720\n",
      "Training completed!\n",
      "\n",
      "Epoch [5/5] Summary:\n",
      "  Training Loss:     0.4445\n",
      "  Validation Loss:   0.6453\n",
      "  Validation Acc:    0.6720\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SCHRITT 4: TRAINING AUSFÜHREN\n",
    "# Trainiert das NCF-Modell mit train_model Methode\n",
    "# ============================================================================\n",
    "\n",
    "# Trainiere das Modell / Train the model\n",
    "TrainingLossHistory, ValidationLossHistory = NCFModel.train_model(\n",
    "    TrainDataLoader=TrainDataLoader,\n",
    "    NumEpochs=5,\n",
    "    Device=Device,\n",
    "    ValidationDataLoader=TestDataLoader,\n",
    "    PrintEverySteps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85de3c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 Empfehlungen für Benutzer 0:\n",
      "    1. Item   7 → Konfidenz: 0.6506\n",
      "    2. Item 322 → Konfidenz: 0.6398\n",
      "    3. Item 325 → Konfidenz: 0.6383\n",
      "    4. Item 225 → Konfidenz: 0.6367\n",
      "    5. Item 335 → Konfidenz: 0.6358\n",
      "    6. Item  25 → Konfidenz: 0.6285\n",
      "    7. Item 326 → Konfidenz: 0.6283\n",
      "    8. Item 129 → Konfidenz: 0.6249\n",
      "    9. Item 356 → Konfidenz: 0.6235\n",
      "   10. Item 479 → Konfidenz: 0.6195\n",
      "Top-10 Empfehlungen für Benutzer 50:\n",
      "    1. Item   7 → Konfidenz: 0.6710\n",
      "    2. Item 325 → Konfidenz: 0.6637\n",
      "    3. Item 225 → Konfidenz: 0.6610\n",
      "    4. Item 322 → Konfidenz: 0.6588\n",
      "    5. Item 335 → Konfidenz: 0.6577\n",
      "    6. Item 326 → Konfidenz: 0.6538\n",
      "    7. Item  25 → Konfidenz: 0.6517\n",
      "    8. Item 129 → Konfidenz: 0.6487\n",
      "    9. Item 479 → Konfidenz: 0.6442\n",
      "   10. Item 356 → Konfidenz: 0.6408\n",
      "Top-10 Empfehlungen für Benutzer 100:\n",
      "    1. Item   7 → Konfidenz: 0.6526\n",
      "    2. Item 322 → Konfidenz: 0.6407\n",
      "    3. Item 325 → Konfidenz: 0.6405\n",
      "    4. Item 225 → Konfidenz: 0.6394\n",
      "    5. Item 335 → Konfidenz: 0.6378\n",
      "    6. Item 326 → Konfidenz: 0.6329\n",
      "    7. Item  25 → Konfidenz: 0.6299\n",
      "    8. Item 129 → Konfidenz: 0.6270\n",
      "    9. Item 479 → Konfidenz: 0.6221\n",
      "   10. Item 213 → Konfidenz: 0.6196\n",
      "Top-10 Empfehlungen für Benutzer 500:\n",
      "    1. Item   7 → Konfidenz: 0.6333\n",
      "    2. Item 322 → Konfidenz: 0.6202\n",
      "    3. Item 225 → Konfidenz: 0.6197\n",
      "    4. Item 335 → Konfidenz: 0.6147\n",
      "    5. Item 325 → Konfidenz: 0.6144\n",
      "    6. Item 326 → Konfidenz: 0.5886\n",
      "    7. Item  25 → Konfidenz: 0.5848\n",
      "    8. Item 129 → Konfidenz: 0.5511\n",
      "    9. Item 479 → Konfidenz: 0.5438\n",
      "   10. Item  64 → Konfidenz: 0.5112\n",
      "Top-10 Empfehlungen für Benutzer 999:\n",
      "    1. Item   7 → Konfidenz: 0.6628\n",
      "    2. Item 322 → Konfidenz: 0.6500\n",
      "    3. Item 325 → Konfidenz: 0.6493\n",
      "    4. Item 225 → Konfidenz: 0.6476\n",
      "    5. Item 335 → Konfidenz: 0.6463\n",
      "    6. Item 326 → Konfidenz: 0.6417\n",
      "    7. Item  25 → Konfidenz: 0.6411\n",
      "    8. Item 129 → Konfidenz: 0.6347\n",
      "    9. Item 479 → Konfidenz: 0.6324\n",
      "   10. Item 356 → Konfidenz: 0.6289\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SCHRITT 5: EMPFEHLUNGEN GENERIEREN FÜR BEISPIELBENUTZER\n",
    "# Demonstration der Top-10 Empfehlungen mit recommend Methode\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "# Wähle 5 Beispielbenutzer aus / Select 5 example users\n",
    "BeispielBenutzer = [0, 50, 100, 500, 999]  # Example users\n",
    "\n",
    "for BenutzerID in BeispielBenutzer:\n",
    "    print(f\"Top-10 Empfehlungen für Benutzer {BenutzerID}:\")    \n",
    "    # Generiere Top-10 Empfehlungen mit recommend Methode\n",
    "    # Generate top-10 recommendations using recommend method\n",
    "    Empfehlungen = NCFModel.recommend(\n",
    "        UserID=BenutzerID,\n",
    "        NumItems=NumItems,\n",
    "        TopK=10,\n",
    "        Device=Device\n",
    "    )\n",
    "    \n",
    "    # Zeige Empfehlungen mit Scores / Show recommendations with scores\n",
    "    for Rang, (ItemID, Score) in enumerate(Empfehlungen, start=1):\n",
    "        print(f\"   {Rang:2d}. Item {ItemID:3d} → Konfidenz: {Score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9dbf0d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
